In-context function estimation is the task of estimating the underlying function given a set of context points that have been sampled from that function or an empirical process. Using the estimated function, it is possible to impute missing data points. \cite{seifner2025zeroshotimputationfoundationinference}

This can be done using an underlying model assumption like with linear regression. A polynomial of a certain order is fitted to the given data, determining which parameters fit best to the data. For linear regression, it is assumed that the underlying function can be modeled by a polynomial. The order of this polynomial must be determined in advance. Due to the best-fit approach, linear regression is not able to handle uncertainties that might result from missing data.

Gaussian Processes (GP) are a way of estimating missing values that takes uncertainties into account. GPs assume a prior distribution of possible underlying functions and update that belief based on the given context points. This has to be done individually for each data frame. \cite{garnelo2018neural}

\autoref{fig:gaussian} shows some results of fitting Gaussian Processes on given context points while knowing the prior distribution from which the true underlying function has been sampled. This posterior distribution defines a mean and standard deviation value for each point in the grid. In the leftmost plot, it is visible that missing data from about $x=0.6$ to $x=1$ leads to high uncertainty. The rightmost plot shows uncertainty spikes between context points. This is due to the low RBF-Score. Sampling with a low RBF Score results in strongly varying functions. Therefore, the GP assumes that the function value can vary a lot, resulting in uncertainty spikes even between context points that are comparatively close on the x-axis. TODO: verify this statement 

\begin{figure*}
	\centering
	\resizebox{0.90\textwidth}{!}{
		\input{figures/plots/GP/63}
		\input{figures/plots/GP/62}
		\input{figures/plots/GP/63_100}
	}
	\caption{Gaussian Process function estimation on four example data frames with varying number of context points. Functions have been sampled from a multivariate normal distribution with an RBF kernel.}
	\label{fig:gaussian}
\end{figure*}

For real-world scenarios, we cannot assume to know the prior distribution beforehand. Rather, we want to work with unseen functions directly without any prior knowledge, which inevitably introduces bias. We need to have a model that estimates the underlying function given only some context points and can impute missing values based on that. This in-context zero-shot function estimation can be split up into two subtasks. First, process the context points so that we get a representation of the underlying function and then predict the value of that function at a certain point.

\citet{Lu_2021} train two neural networks for their DeepONet, that tackles these subtasks. The Branch net learns an operator $G: u\rightarrow G(u)$ with $u$ being the sequence of context points and the result $G(u)$ is supposed to represent the underlying function. Using this representation and given an arbitrary point $y$, the Trunk net predicts the value $G(u)(y)$. \cite{Lu_2021}

Neural networks are not well fit to handle sequences of varying length, as we expect with the sequence of context points having a different length depending on the specific use case. \citet{seifner2025zeroshotimputationfoundationinference} use a bi-directional LSTM that acts like the Branch net from DeepONets and allows processing sequences of varying length. Adapting their approach, we use a transformer to model sequences of varying length.