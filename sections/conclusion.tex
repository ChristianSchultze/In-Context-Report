This project demonstrates how to train a transformer based model for in-context function estimation with synthetic 1d-time series data and apply this model successfully to real world data. We show that our approach outperforms Gaussian process fitted on the same data, despite the GP knowing the true prior distribution. For both GPs and our model data with strongly varying values or large noise levels pose a challenge. It should be further examined whether this is due insufficient information caused by a low resolution grid, or resulting from a less than optimal training process.