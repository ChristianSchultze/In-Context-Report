This project demonstrates how to train a transformer-based model for in-context function estimation with synthetic 1d-time series data and we apply this model successfully to real-world data. We show that our approach outperforms the Gaussian Process fitted on the same data, despite the GP knowing the true prior distribution. For both GPs and our model, data with strongly varying values or large noise levels pose a challenge. It should be further examined whether this is due to insufficient information caused by a low resolution grid, or resulting from a less than optimal training process.