For evaluation we use eight test datasets (four for each version) with fixed number of context points (\autoref{tab:fun_subset}). We use the posterior distribution of a Gaussian Process as baseline. The GP has access to the true prior distribution from which the data has been sampled. All models are evaluated on all 1.000 functions in each dataset.
As shown in \autoref{tag:results} both models outperform the GP baseline. One reason for this are the uncertainty spikes visible in \autoref{fig:gp_comp}. This happens for RBF Scales below 0.1 and context points with too much noise. while the results of model A and B are very similar. For all models we observe increasing performance with increasing number of context points. This is clearly visible in \autoref{fig:result_plot}, with the left and middle plot showing a lot of uncertainty due to the low number of context points. The right plot shows the prediction much closer to the target despite a large amount of noise. Furthermore evaluations on version B datasets show better performance than evaluation on version A datasets. This is expected, as dataset A is more challenging. Model B showing similar performance to model A on dataset A, despite being trained on the less challenging dataset B is a sign of good generalization capabilities. However, as shown in \autoref{fig:beta_scales}, the RBF Scale distributions overlap strongly. For this reason, model B has seen sufficient examples of challenging functions during training and the similar performance cannot be attributed to generalization.

	\begin{table}[]
	\caption{Evaluation results on all eight test dataset with RMSE Loss for mean compared to target values}
	\begin{tabular}{c c c c c}
		\toprule
		Dataset & Context Points & GP & model A & model B\\
		\midrule
		\multirow{4}{*}{A (2,10)} & 5 & 0.91 & 0.72 & 0.71\\
		&10& 0.78 & 0.50 & 0.49\\ 
		&50 & 0.51 & 0.26 & 0.29\\
		&100 & 0.32 & 0.24 & 0.28\\\midrule
		\multirow{4}{*}{B (2,5)} & 5 & 0.84 & 0.55 & 0.55\\
		& 10 & 0.63 & 0.37 & 0.36\\
		& 50 & 0.28 & 0.14 & 0.16\\
		& 100 & 0.16 & 0.13 & 0.16\\\bottomrule
	\end{tabular}
\label{tag:results}
\end{table}

	\begin{figure*}
	\centering
	\resizebox{0.7\textwidth}{!}{
		\input{figures/plots/comparison/51}
		\input{figures/plots/comparison/63}
	}
\label{fig:gp_comp}
\caption{Comparison plots between model A and the GP baseline.}
\end{figure*}

	\begin{figure*}
	\centering
	\resizebox{0.9\textwidth}{!}{
		\input{figures/plots/modelA/5/11}
		\input{figures/plots/modelA/10/9}
		\input{figures/plots/modelA/100/3}
	}
\label{fig:result_plot}
\caption{Model A prediction plots with increasing number of context points (5,10,50).}
\end{figure*}

Finally, we process cepheid star luminosity data. \autoref{fig:cepheid} shows prediction results for model A. Due to missing ground truth data we cannot evaluate the performance numerically. We observe a comparatively high uncertainty despite a high number of context points especially during the low luminosity phases. Otherwise we are satisfied with the results that show good generalization from training on synthetic data and applying the same model on real world observations.

\begin{figure*}
	\centering
	\resizebox{0.6\textwidth}{!}{
		\input{figures/plots/modelA/eval_Cepheid/8}
		\input{figures/plots/modelA/eval_Cepheid/11}
	}
\label{fig:cepheid}
\caption{Model A Prediction result on luminosity values (unit flux) for a cepheid variable star over 24 hours. Data from Benk≈ë et al. \cite{Benk__2014}}
\end{figure*}